# -*- coding: utf-8 -*-
"""Final_From_Arabic_Dialects_to_MSA_and_English_Transforamation_last_version (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YK7LDDWIRxsKEq33aGZahEJDXA7KhUmk

# **Getting Data**
"""

from google.colab import drive
drive.mount('/content/drive')

!gdown 1e-PgbDmU-aHerGNftuM38IeDFBYthCst
!gdown 1MTI5zvQwnMNRLBYDTRnwS--Xq47WuSvE
!gdown 1T1MuT3-nQStGl0K1jFyKsIptLIfKeHlP
!gdown 16N2YAoIdQ2tGVa6A-6qAi8hTwJs6oIyY
!gdown 1xwoKclfvbkDyHBbLaMOGKCF1yQrM-Q6A

import pandas as pd
original_data = pd.read_csv('longest_captions_dataset -1500 samples.csv')
egyptian_data = pd.read_csv('cleaned_egyptain_dialect_data.csv')
morrocon_data = pd.read_csv('morrocon_dialect_after_cleaning.csv')
khaleji_data = pd.read_csv('cleaned_khaleeji_dialect_data.csv')
saudi_data = pd.read_csv('saudi_arabia_dialect_after_cleaning.csv')

#original_data.head()
#egyptian_data.head()
#saudi_data.head()
morrocon_data.head()

"""# **Model 1: PRAli22/arat5**"""

# Load model directly
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
tokenizer = AutoTokenizer.from_pretrained("PRAli22/arat5-arabic-dialects-translation")
model_1 = AutoModelForSeq2SeqLM.from_pretrained("PRAli22/arat5-arabic-dialects-translation")

# create function to transform text for model 1 atat5
def transform_text_model_1(input_text):
    inputs = tokenizer(input_text, return_tensors="pt")
    outputs = model_1.generate(**inputs)
    # Decode the outputs and print the results
    decoded_sentences = tokenizer.batch_decode(outputs, skip_special_tokens=True)
    return decoded_sentences[0]

transform_text_model_1(" رجال نايم على دكة برة مع كلب أبيض جالس جنبه ")

"""## **Main function to apply transformation on multiple dataframes**"""

from tqdm import tqdm
def apply_transformation_and_save(df, func, filename, column_name, save_interval=100):
  """Applies a transformation function to a dataframe and saves the results incrementally.

  Args:
    df: The dataframe to process.
    func: The transformation function to apply.
    save_interval: The number of rows to process before saving.
    filename: The filename for the saved data.
  """
  transformed_data = []
  for index, row in tqdm(df.iterrows(), total=df.shape[0]):
    try:
      transformed_text = func(row[column_name])
      transformed_data.append([row[column_name], transformed_text])
      if (index + 1) % save_interval == 0:
        new_df = pd.DataFrame(transformed_data, columns=['original_text', 'transformed_text'])
        new_df.to_csv(filename, mode='a', header=not index == 0, index=False)  # Append to file, add header only on first iteration
        transformed_data = []  # Clear list to avoid memory overflow
    except Exception as e:
      print(f"Error processing row {index}: {e}")

# apply for egyptian data
apply_transformation_and_save(egyptian_data, transform_text_model_1, "egyptian_data_transformed_model_1.csv", "Egyptian")

apply_transformation_and_save(saudi_data, transform_text_model_1, "saudi_data_transformed_model_1.csv", "Khaleeji")

apply_transformation_and_save(morrocon_data, transform_text_model_1, "morrocon_data_transformed_model_1.csv", "Morrocon")

apply_transformation_and_save(khaleji_data, transform_text_model_1, "khaleji_data_transformed_model_1.csv", "Khaleeji")

"""# **Model 2 Murhaf/AraT5**"""

# Load model directly
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
model_name = "Murhaf/AraT5-MSAizer"
tokenizer_2 = AutoTokenizer.from_pretrained(model_name)
model_2 = AutoModelForSeq2SeqLM.from_pretrained(model_name)

# create function to transform text for model 1 atat5
def transform_text_model_2(input_text):
    inputs = tokenizer_2(input_text, return_tensors="pt")
    outputs = model_2.generate(**inputs)
    # Decode the outputs and print the results
    decoded_sentences = tokenizer_2.batch_decode(outputs, skip_special_tokens=True)
    return decoded_sentences[0]

transform_text_model_2(" رجال نايم على دكة برة مع كلب أبيض جالس جنبه ")

"""## **Main function to apply transformation on multiple dataframes**"""

from tqdm import tqdm
def apply_transformation_and_save(df, func, filename, column_name, save_interval=100):
  """Applies a transformation function to a dataframe and saves the results incrementally.

  Args:
    df: The dataframe to process.
    func: The transformation function to apply.
    save_interval: The number of rows to process before saving.
    filename: The filename for the saved data.
  """
  transformed_data = []
  for index, row in tqdm(df.iterrows(), total=df.shape[0]):
    try:
      transformed_text = func(row[column_name])
      transformed_data.append([row[column_name], transformed_text])
      if (index + 1) % save_interval == 0:
        new_df = pd.DataFrame(transformed_data, columns=['original_text', 'transformed_text'])
        new_df.to_csv(filename, mode='a', header=not index == 0, index=False)  # Append to file, add header only on first iteration
        transformed_data = []  # Clear list to avoid memory overflow
    except Exception as e:
      print(f"Error processing row {index}: {e}")

# apply for egyptian data
apply_transformation_and_save(egyptian_data, transform_text_model_2, "egyptian_data_transformed_model_2.csv", "Egyptian")

apply_transformation_and_save(saudi_data, transform_text_model_2, "saudi_data_transformed_model_2.csv", "Khaleeji")

apply_transformation_and_save(morrocon_data, transform_text_model_2, "morrocon_data_transformed_model_2.csv", "Morrocon")

apply_transformation_and_save(khaleji_data, transform_text_model_2, "khaleji_data_transformed_model_2.csv", "Khaleeji")

"""# **Model 3 English Transformation: nadsoft/Faseeh**"""

# Load model directly
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
tokenizer_3 = AutoTokenizer.from_pretrained("nadsoft/Faseeh-v0.1-beta")
model_3 = AutoModelForSeq2SeqLM.from_pretrained("nadsoft/Faseeh-v0.1-beta")

# create function to transform text for model 1 atat5
def transform_text_model_3(input_text):
    inputs = tokenizer_3(input_text, return_tensors="pt")
    outputs = model_3.generate(**inputs)
    # Decode the outputs and print the results
    decoded_sentences = tokenizer_3.batch_decode(outputs, skip_special_tokens=True)
    return decoded_sentences[0]

transform_text_model_3(" رجال نايم على دكة برة مع كلب شقر وكحلي جالس جنبه ")

"""## **Main function to apply transformation on multiple dataframes**"""

# apply for egyptian data
apply_transformation_and_save(egyptian_data, transform_text_model_3, "egyptian_data_transformed_model_3.csv", "Egyptian")

apply_transformation_and_save(saudi_data, transform_text_model_3, "saudi_data_transformed_model_3.csv", "Khaleeji")

apply_transformation_and_save(morrocon_data, transform_text_model_3, "morrocon_data_transformed_model_3.csv", "Morrocon")

apply_transformation_and_save(khaleji_data, transform_text_model_3, "khaleji_data_transformed_model_3.csv", "Khaleeji")

"""# **Model 4 : Gemini Arabic**"""

!gdown 1G-OBSp5s0NKR2HwnpXcI52Gv1rNcZGcC

!unzip /content/transformer_dialects_data.zip

!pip install langchain_google_genai
!pip install langchain

from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.prompts import PromptTemplate
import google.generativeai as genai
import pandas as pd

model = genai.GenerativeModel(model_name = "gemini-1.5-flash-latest")

google_api_key=''#Put your API KEY

"""## Egyptain"""

egyptian_data=pd.read_csv('/content/cleaned_egyptain_dialect_data.csv')

egyptian_data.head()

from langchain.schema import HumanMessage

"""#### Dialects Text Generation"""

def generate_response(sentences):
    llm = ChatGoogleGenerativeAI(model="gemini-1.5-flash-latest",google_api_key=google_api_key)
    template="""""
اعمل كمترجم من اللهجه العاميه المصريه  إلى اللغه العربيه الفصحى وأجب فقط بالشكل المطلوب دون إضافة كلمات إضافية.
     و ملاحظة: كل جملة بالهجه العاميه المصريه مرقمة، أعد الجملة المترجمة إلى اللغه العربيه الفصحى بنفس الترقيم، لا تضف أو تحذف أي شيء و اضف سطر جديد بعد كل جمله و الاخرى.
    الجمل بالاللهجه العاميه المصريه:
{arabic_sentences}
اللغه العربيه الفصحى:
"""

    prompt_template = PromptTemplate(
        input_variables=["arabic_sentences"],
        template=template
    )
    ques=prompt_template.format(
            arabic_sentences=sentences
        )
    result = llm([HumanMessage(content=ques)])

    # Process the response and extract the three dialect outputs separately
    response = result.content
    return response

egyptian_data.head()

# numbring the sentences
egyptian_data['Egyptian'] = egyptian_data.index.to_series().apply(str) + '-' + egyptian_data['Egyptian']
egyptian_data.head()

egyptian_data.tail()

egyptian_data.shape

chunk = egyptian_data['Egyptian'][0:50]
chunk_string = " ".join(chunk.astype(str))
response = generate_response(chunk_string)
response

import pandas as pd
import os
import time
from langchain.prompts import PromptTemplate
from langchain.schema import HumanMessage

# Function to split response by newline or numbered prefix
def split_response(response):
    # Split by numbered prefix or newline
    return [sent.strip() for sent in response.splitlines() if sent.strip()]

# Define your Google Drive CSV file path
output_file_path = '/content/drive/MyDrive/Final V Project 283/Text_transformer_dialects_data/Gemini to MSA data/egyptain_msa_gemini.csv'


# Process in chunks of 50 and append results to CSV
for i in range(0, len(egyptian_data), 50):
    # Sleep for rate-limiting purposes
    if i % 3 == 0:
        time.sleep(30)

    # Get chunk of 50 sentences
    chunk = egyptian_data['Egyptian'][i:i+50]
    chunk_string = " ".join(chunk.astype(str))  # Convert to a single string

    # Generate the response
    response = generate_response(chunk_string)

    # Split the response into individual lines (by newline or numbered prefix)
    response_list = split_response(response)

    # Convert to DataFrame for CSV storage
    df_results = pd.DataFrame(response_list, columns=['Egyptian'])

    # Write to CSV
    if i == 0:
        df_results.to_csv(output_file_path, mode='w', index=False)  # Write with header
    else:
        df_results.to_csv(output_file_path, mode='a', index=False, header=False)  # Append without header

    print(f"Processed chunk {i} to {i+50}")

print(f"All data has been processed and saved to {output_file_path}.")



"""## Morocan"""

morrocon_data=pd.read_csv('/content/morrocon_dialect_after_cleaning.csv')

morrocon_data.head()

from langchain.schema import HumanMessage

"""#### Dialects Text Generation"""

def generate_response(sentences):
    llm = ChatGoogleGenerativeAI(model="gemini-1.5-flash-latest",google_api_key=google_api_key)
    template="""""
اعمل كمترجم من اللهجه العاميه المغربيه  إلى اللغه العربيه الفصحى وأجب فقط بالشكل المطلوب دون إضافة كلمات إضافية.
     و ملاحظة: كل جملة باللهجه العاميه المغربيه مرقمة، أعد الجملة المترجمة إلى اللغه العربيه الفصحى بنفس الترقيم، لا تضف أو تحذف أي شيء و اضف سطر جديد بعد كل جمله و الاخرى.
    الجمل بالاللهجه العاميه المغربيه:
{arabic_sentences}
اللغه العربيه الفصحى:
"""

    prompt_template = PromptTemplate(
        input_variables=["arabic_sentences"],
        template=template
    )
    ques=prompt_template.format(
            arabic_sentences=sentences
        )
    result = llm([HumanMessage(content=ques)])

    # Process the response and extract the three dialect outputs separately
    response = result.content
    return response

morrocon_data.head()

# numbring the sentences
morrocon_data['Morrocon'] = morrocon_data.index.to_series().apply(str) + '-' + morrocon_data['Morrocon']
morrocon_data.head()

morrocon_data.tail()

#egyptian_data = egyptian_data[:-2]

morrocon_data.shape

chunk = morrocon_data['Morrocon'][0:50]
chunk_string = " ".join(chunk.astype(str))
response = generate_response(chunk_string)
response

import pandas as pd
import os
import time
from langchain.prompts import PromptTemplate
from langchain.schema import HumanMessage

# Function to split response by newline or numbered prefix
def split_response(response):
    # Split by numbered prefix or newline
    return [sent.strip() for sent in response.splitlines() if sent.strip()]

# Define your Google Drive CSV file path
output_file_path = '/content/drive/MyDrive/Final V Project 283/Text_transformer_dialects_data/Gemini to MSA data/moroccan_msa_gemini.csv'


# Process in chunks of 50 and append results to CSV
for i in range(0, len(egyptian_data), 50):
    # Sleep for rate-limiting purposes
    if i % 3 == 0:
        time.sleep(30)

    # Get chunk of 50 sentences
    chunk = morrocon_data['Morrocon'][i:i+50]
    chunk_string = " ".join(chunk.astype(str))  # Convert to a single string

    # Generate the response
    response = generate_response(chunk_string)

    # Split the response into individual lines (by newline or numbered prefix)
    response_list = split_response(response)

    # Convert to DataFrame for CSV storage
    df_results = pd.DataFrame(response_list, columns=['Morrocon'])

    # Write to CSV
    if i == 0:
        df_results.to_csv(output_file_path, mode='w', index=False)  # Write with header
    else:
        df_results.to_csv(output_file_path, mode='a', index=False, header=False)  # Append without header

    print(f"Processed chunk {i} to {i+50}")

print(f"All data has been processed and saved to {output_file_path}.")

"""## saudi arabia"""

saudi_arabia_data=pd.read_csv('/content/saudi_arabia_dialect_after_cleaning.csv')

saudi_arabia_data.head()

from langchain.schema import HumanMessage

"""#### Dialects Text Generation"""

def generate_response(sentences):
    llm = ChatGoogleGenerativeAI(model="gemini-1.5-flash-latest",google_api_key=google_api_key)
    template="""""
اعمل كمترجم من اللهجه السعوديه إلى اللغه العربيه الفصحى وأجب فقط بالشكل المطلوب دون إضافة كلمات إضافية.
     و ملاحظة: كل جملة اللهجه السعوديه مرقمة، أعد الجملة المترجمة إلى اللغه العربيه الفصحى بنفس الترقيم، لا تضف أو تحذف أي شيء و اضف سطر جديد بعد كل جمله و الاخرى.
    الجمل باللهجه السعوديه:
{arabic_sentences}
اللغه العربيه الفصحى:
"""

    prompt_template = PromptTemplate(
        input_variables=["arabic_sentences"],
        template=template
    )
    ques=prompt_template.format(
            arabic_sentences=sentences
        )
    result = llm([HumanMessage(content=ques)])

    # Process the response and extract the three dialect outputs separately
    response = result.content
    return response

saudi_arabia_data.head()

# numbring the sentences
saudi_arabia_data['Khaleeji'] = saudi_arabia_data.index.to_series().apply(str) + '-' + saudi_arabia_data['Khaleeji']
saudi_arabia_data.head()

saudi_arabia_data.tail()

saudi_arabia_data.shape

chunk = saudi_arabia_data['Khaleeji'][0:50]
chunk_string = " ".join(chunk.astype(str))
response = generate_response(chunk_string)
response

import pandas as pd
import os
import time
from langchain.prompts import PromptTemplate
from langchain.schema import HumanMessage

# Function to split response by newline or numbered prefix
def split_response(response):
    # Split by numbered prefix or newline
    return [sent.strip() for sent in response.splitlines() if sent.strip()]

# Define your Google Drive CSV file path
output_file_path = '/content/drive/MyDrive/Final V Project 283/Text_transformer_dialects_data/Gemini to MSA data/saudi_arabia_msa_gemini.csv'


# Process in chunks of 50 and append results to CSV
for i in range(0, len(saudi_arabia_data), 50):
    # Sleep for rate-limiting purposes
    if i % 3 == 0:
        time.sleep(30)

    # Get chunk of 50 sentences
    chunk = saudi_arabia_data['Khaleeji'][i:i+50]
    chunk_string = " ".join(chunk.astype(str))  # Convert to a single string

    # Generate the response
    response = generate_response(chunk_string)

    # Split the response into individual lines (by newline or numbered prefix)
    response_list = split_response(response)

    # Convert to DataFrame for CSV storage
    df_results = pd.DataFrame(response_list, columns=['Khaleeji'])

    # Write to CSV
    if i == 0:
        df_results.to_csv(output_file_path, mode='w', index=False)  # Write with header
    else:
        df_results.to_csv(output_file_path, mode='a', index=False, header=False)  # Append without header

    print(f"Processed chunk {i} to {i+50}")

print(f"All data has been processed and saved to {output_file_path}.")

"""## Khaleji dialect"""

khaleji_data=pd.read_csv('/content/cleaned_khaleeji_dialect_data.csv')

khaleji_data.head()

from langchain.schema import HumanMessage

"""#### Dialects Text Generation"""

def generate_response(sentences):
    llm = ChatGoogleGenerativeAI(model="gemini-1.5-flash-latest",google_api_key=google_api_key)
    template="""""
اعمل كمترجم من اللهجه الخليجيه  إلى اللغه العربيه الفصحى وأجب فقط بالشكل المطلوب دون إضافة كلمات إضافية.
     و ملاحظة: كل جملة باللهجه الخليجيه مرقمة، أعد الجملة المترجمة إلى اللغه العربيه الفصحى بنفس الترقيم، لا تضف أو تحذف أي شيء و اضف سطر جديد بعد كل جمله و الاخرى.
    الجمل باللهجه الخليجيه:
{arabic_sentences}
اللغه العربيه الفصحى:
"""

    prompt_template = PromptTemplate(
        input_variables=["arabic_sentences"],
        template=template
    )
    ques=prompt_template.format(
            arabic_sentences=sentences
        )
    result = llm([HumanMessage(content=ques)])

    # Process the response and extract the three dialect outputs separately
    response = result.content
    return response

khaleji_data.head()

# numbring the sentences
khaleji_data['Khaleeji'] = khaleji_data.index.to_series().apply(str) + '-' + khaleji_data['Khaleeji']
khaleji_data.head()

khaleji_data.tail()

khaleji_data.shape

chunk = khaleji_data['Khaleeji'][0:50]
chunk_string = " ".join(chunk.astype(str))
response = generate_response(chunk_string)
response

import pandas as pd
import os
import time
from langchain.prompts import PromptTemplate
from langchain.schema import HumanMessage

# Function to split response by newline or numbered prefix
def split_response(response):
    # Split by numbered prefix or newline
    return [sent.strip() for sent in response.splitlines() if sent.strip()]

# Define your Google Drive CSV file path
output_file_path = '/content/drive/MyDrive/Final V Project 283/Text_transformer_dialects_data/Gemini to MSA data/khaleji_msa_gemini.csv'


# Process in chunks of 50 and append results to CSV
for i in range(0, len(khaleji_data), 50):
    # Sleep for rate-limiting purposes
    if i % 3 == 0:
        time.sleep(30)

    # Get chunk of 50 sentences
    chunk = khaleji_data['Khaleeji'][i:i+50]
    chunk_string = " ".join(chunk.astype(str))  # Convert to a single string

    # Generate the response
    response = generate_response(chunk_string)

    # Split the response into individual lines (by newline or numbered prefix)
    response_list = split_response(response)

    # Convert to DataFrame for CSV storage
    df_results = pd.DataFrame(response_list, columns=['Khaleeji'])

    # Write to CSV
    if i == 0:
        df_results.to_csv(output_file_path, mode='w', index=False)  # Write with header
    else:
        df_results.to_csv(output_file_path, mode='a', index=False, header=False)  # Append without header

    print(f"Processed chunk {i} to {i+50}")

print(f"All data has been processed and saved to {output_file_path}.")

"""# **Model 5 : Gemini English**

## Egyptain
"""

egyptian_data=pd.read_csv('/content/cleaned_egyptain_dialect_data.csv')

egyptian_data.head()

from langchain.schema import HumanMessage

"""#### Dialects Text Generation"""

def generate_response(sentences):
    llm = ChatGoogleGenerativeAI(model="gemini-1.5-flash-latest",google_api_key=google_api_key)
    template="""""
اعمل كمترجم من اللهجه العاميه المصريه  إلى اللغه الانجليزيه وأجب فقط بالشكل المطلوب دون إضافة كلمات إضافية.
     و ملاحظة: كل جملة بالهجه العاميه المصريه مرقمة، أعد الجملة المترجمة إلى اللغه الانجليزيه بنفس الترقيم، لا تضف أو تحذف أي شيء و اضف سطر جديد بعد كل جمله و الاخرى.
    الجمل بالاللهجه العاميه المصريه:
{arabic_sentences}
اللغه الانجليزيه:
"""

    prompt_template = PromptTemplate(
        input_variables=["arabic_sentences"],
        template=template
    )
    ques=prompt_template.format(
            arabic_sentences=sentences
        )
    result = llm([HumanMessage(content=ques)])

    # Process the response and extract the three dialect outputs separately
    response = result.content
    return response

egyptian_data.head()

# numbring the sentences
egyptian_data['Egyptian'] = egyptian_data.index.to_series().apply(str) + '-' + egyptian_data['Egyptian']
egyptian_data.head()

egyptian_data.tail()

egyptian_data.shape

chunk = egyptian_data['Egyptian'][0:50]
chunk_string = " ".join(chunk.astype(str))
response = generate_response(chunk_string)
response

import pandas as pd
import os
import time
from langchain.prompts import PromptTemplate
from langchain.schema import HumanMessage

# Function to split response by newline or numbered prefix
def split_response(response):
    # Split by numbered prefix or newline
    return [sent.strip() for sent in response.splitlines() if sent.strip()]

# Define your Google Drive CSV file path
output_file_path = '/content/drive/MyDrive/Final V Project 283/Text_transformer_dialects_data/Gemini to English data/egyptain_english_gemini.csv'


# Process in chunks of 50 and append results to CSV
for i in range(0, len(egyptian_data), 50):
    # Sleep for rate-limiting purposes
    if i % 3 == 0:
        time.sleep(30)

    # Get chunk of 50 sentences
    chunk = egyptian_data['Egyptian'][i:i+50]
    chunk_string = " ".join(chunk.astype(str))  # Convert to a single string

    # Generate the response
    response = generate_response(chunk_string)

    # Split the response into individual lines (by newline or numbered prefix)
    response_list = split_response(response)

    # Convert to DataFrame for CSV storage
    df_results = pd.DataFrame(response_list, columns=['Egyptian'])

    # Write to CSV
    if i == 0:
        df_results.to_csv(output_file_path, mode='w', index=False)  # Write with header
    else:
        df_results.to_csv(output_file_path, mode='a', index=False, header=False)  # Append without header

    print(f"Processed chunk {i} to {i+50}")

print(f"All data has been processed and saved to {output_file_path}.")



"""## Morocan"""

morrocon_data=pd.read_csv('/content/morrocon_dialect_after_cleaning.csv')

morrocon_data.head()

from langchain.schema import HumanMessage

"""#### Dialects Text Generation"""

def generate_response(sentences):
    llm = ChatGoogleGenerativeAI(model="gemini-1.5-flash-latest",google_api_key=google_api_key)
    template="""""
اعمل كمترجم من اللهجه العاميه المغربيه  إلى اللغه الانجليزيه وأجب فقط بالشكل المطلوب دون إضافة كلمات إضافية.
     و ملاحظة: كل جملة باللهجه العاميه المغربيه مرقمة، أعد الجملة المترجمة إلى اللغه الانجليزيه بنفس الترقيم، لا تضف أو تحذف أي شيء و اضف سطر جديد بعد كل جمله و الاخرى.
    الجمل بالاللهجه العاميه المغربيه:
{arabic_sentences}
اللغه الانجليزيه:
"""

    prompt_template = PromptTemplate(
        input_variables=["arabic_sentences"],
        template=template
    )
    ques=prompt_template.format(
            arabic_sentences=sentences
        )
    result = llm([HumanMessage(content=ques)])

    # Process the response and extract the three dialect outputs separately
    response = result.content
    return response

morrocon_data.head()

# numbring the sentences
morrocon_data['Morrocon'] = morrocon_data.index.to_series().apply(str) + '-' + morrocon_data['Morrocon']
morrocon_data.head()

morrocon_data.tail()

#egyptian_data = egyptian_data[:-2]

morrocon_data.shape

chunk = morrocon_data['Morrocon'][0:50]
chunk_string = " ".join(chunk.astype(str))
response = generate_response(chunk_string)
response

import pandas as pd
import os
import time
from langchain.prompts import PromptTemplate
from langchain.schema import HumanMessage

# Function to split response by newline or numbered prefix
def split_response(response):
    # Split by numbered prefix or newline
    return [sent.strip() for sent in response.splitlines() if sent.strip()]

# Define your Google Drive CSV file path
output_file_path = '/content/drive/MyDrive/Final V Project 283/Text_transformer_dialects_data/Gemini to English data/moroccan_english_gemini.csv'


# Process in chunks of 50 and append results to CSV
for i in range(0, len(egyptian_data), 50):
    # Sleep for rate-limiting purposes
    if i % 3 == 0:
        time.sleep(30)

    # Get chunk of 50 sentences
    chunk = morrocon_data['Morrocon'][i:i+50]
    chunk_string = " ".join(chunk.astype(str))  # Convert to a single string

    # Generate the response
    response = generate_response(chunk_string)

    # Split the response into individual lines (by newline or numbered prefix)
    response_list = split_response(response)

    # Convert to DataFrame for CSV storage
    df_results = pd.DataFrame(response_list, columns=['Morrocon'])

    # Write to CSV
    if i == 0:
        df_results.to_csv(output_file_path, mode='w', index=False)  # Write with header
    else:
        df_results.to_csv(output_file_path, mode='a', index=False, header=False)  # Append without header

    print(f"Processed chunk {i} to {i+50}")

print(f"All data has been processed and saved to {output_file_path}.")

"""## saudi arabia"""

saudi_arabia_data=pd.read_csv('/content/saudi_arabia_dialect_after_cleaning.csv')

saudi_arabia_data.head()

from langchain.schema import HumanMessage

"""#### Dialects Text Generation"""

def generate_response(sentences):
    llm = ChatGoogleGenerativeAI(model="gemini-1.5-flash-latest",google_api_key=google_api_key)
    template="""""
اعمل كمترجم من اللهجه السعوديه إلى اللغه الانجليزيه وأجب فقط بالشكل المطلوب دون إضافة كلمات إضافية.
     و ملاحظة: كل جملة اللهجه السعوديه مرقمة، أعد الجملة المترجمة إلى اللغه الانجليزيه بنفس الترقيم، لا تضف أو تحذف أي شيء و اضف سطر جديد بعد كل جمله و الاخرى.
    الجمل باللهجه السعوديه :
{arabic_sentences}
اللغه الانجليزيه:
"""

    prompt_template = PromptTemplate(
        input_variables=["arabic_sentences"],
        template=template
    )
    ques=prompt_template.format(
            arabic_sentences=sentences
        )
    result = llm([HumanMessage(content=ques)])

    # Process the response and extract the three dialect outputs separately
    response = result.content
    return response

saudi_arabia_data.head()

# numbring the sentences
saudi_arabia_data['Khaleeji'] = saudi_arabia_data.index.to_series().apply(str) + '-' + saudi_arabia_data['Khaleeji']
saudi_arabia_data.head()

saudi_arabia_data.tail()

saudi_arabia_data.shape

chunk = saudi_arabia_data['Khaleeji'][0:50]
chunk_string = " ".join(chunk.astype(str))
response = generate_response(chunk_string)
response

import pandas as pd
import os
import time
from langchain.prompts import PromptTemplate
from langchain.schema import HumanMessage

# Function to split response by newline or numbered prefix
def split_response(response):
    # Split by numbered prefix or newline
    return [sent.strip() for sent in response.splitlines() if sent.strip()]

# Define your Google Drive CSV file path
output_file_path = '/content/drive/MyDrive/Final V Project 283/Text_transformer_dialects_data/Gemini to English data/saudi_arabia_english_gemini.csv'


# Process in chunks of 50 and append results to CSV
for i in range(0, len(saudi_arabia_data), 50):
    # Sleep for rate-limiting purposes
    if i % 3 == 0:
        time.sleep(30)

    # Get chunk of 50 sentences
    chunk = saudi_arabia_data['Khaleeji'][i:i+50]
    chunk_string = " ".join(chunk.astype(str))  # Convert to a single string

    # Generate the response
    response = generate_response(chunk_string)

    # Split the response into individual lines (by newline or numbered prefix)
    response_list = split_response(response)

    # Convert to DataFrame for CSV storage
    df_results = pd.DataFrame(response_list, columns=['Khaleeji'])

    # Write to CSV
    if i == 0:
        df_results.to_csv(output_file_path, mode='w', index=False)  # Write with header
    else:
        df_results.to_csv(output_file_path, mode='a', index=False, header=False)  # Append without header

    print(f"Processed chunk {i} to {i+50}")

print(f"All data has been processed and saved to {output_file_path}.")

"""## Khaleji dialect"""

khaleji_data=pd.read_csv('/content/cleaned_khaleeji_dialect_data.csv')

khaleji_data.head()

from langchain.schema import HumanMessage

"""#### Dialects Text Generation"""

def generate_response(sentences):
    llm = ChatGoogleGenerativeAI(model="gemini-1.5-flash-latest",google_api_key=google_api_key)
    template="""""
اعمل كمترجم من اللهجه الخليجيه  إلى اللغه الانجليزيه وأجب فقط بالشكل المطلوب دون إضافة كلمات إضافية.
     و ملاحظة: كل جملة باللهجه الخليجيه مرقمة، أعد الجملة المترجمة إلى اللغه الانجليزيه بنفس الترقيم، لا تضف أو تحذف أي شيء و اضف سطر جديد بعد كل جمله و الاخرى.
    الجمل باللهجه الخليجيه:
{arabic_sentences}
اللغه الانجليزيه:
"""

    prompt_template = PromptTemplate(
        input_variables=["arabic_sentences"],
        template=template
    )
    ques=prompt_template.format(
            arabic_sentences=sentences
        )
    result = llm([HumanMessage(content=ques)])

    # Process the response and extract the three dialect outputs separately
    response = result.content
    return response

khaleji_data.head()

# numbring the sentences
khaleji_data['Khaleeji'] = khaleji_data.index.to_series().apply(str) + '-' + khaleji_data['Khaleeji']
khaleji_data.head()

khaleji_data.tail()

khaleji_data.shape

chunk = khaleji_data['Khaleeji'][0:50]
chunk_string = " ".join(chunk.astype(str))
response = generate_response(chunk_string)
response

import pandas as pd
import os
import time
from langchain.prompts import PromptTemplate
from langchain.schema import HumanMessage

# Function to split response by newline or numbered prefix
def split_response(response):
    # Split by numbered prefix or newline
    return [sent.strip() for sent in response.splitlines() if sent.strip()]

# Define your Google Drive CSV file path
output_file_path = '/content/drive/MyDrive/Final V Project 283/Text_transformer_dialects_data/Gemini to English data/khaleji_english_gemini.csv'


# Process in chunks of 50 and append results to CSV
for i in range(0, len(khaleji_data), 50):
    # Sleep for rate-limiting purposes
    if i % 3 == 0:
        time.sleep(30)

    # Get chunk of 50 sentences
    chunk = khaleji_data['Khaleeji'][i:i+50]
    chunk_string = " ".join(chunk.astype(str))  # Convert to a single string

    # Generate the response
    response = generate_response(chunk_string)

    # Split the response into individual lines (by newline or numbered prefix)
    response_list = split_response(response)

    # Convert to DataFrame for CSV storage
    df_results = pd.DataFrame(response_list, columns=['Khaleeji'])

    # Write to CSV
    if i == 0:
        df_results.to_csv(output_file_path, mode='w', index=False)  # Write with header
    else:
        df_results.to_csv(output_file_path, mode='a', index=False, header=False)  # Append without header

    print(f"Processed chunk {i} to {i+50}")

print(f"All data has been processed and saved to {output_file_path}.")

"""### **Clean gemini data**"""

# prompt: I want function that remove numbers and "-" which is at the begging of each row in the data

import pandas as pd
import re

def clean_data(df, column_name):
  df_copy = df.copy()  # Create a copy to avoid modifying the original DataFrame
  df_copy[column_name] = df_copy[column_name].astype(str).apply(lambda x: re.sub(r"^\d+-?", "", x).strip())
  return df_copy

# prompt: read all the data here /content/drive/MyDrive/arabic dialects project faisal/Text_transformer_dialects_data/Gemini to English data and name df with file name

import pandas as pd
import os
import re

def clean_data(df, column_name):
  df_copy = df.copy()  # Create a copy to avoid modifying the original DataFrame
  df_copy[column_name] = df_copy[column_name].astype(str).apply(lambda x: re.sub(r"^\d+-?", "", x).strip())
  return df_copy

# Define the directory path
directory_path = '/content/drive/MyDrive/arabic dialects project faisal/Text_transformer_dialects_data/Gemini to MSA data'

# Iterate through each file in the directory
for filename in os.listdir(directory_path):
    if filename.endswith('.csv'):  # Check if the file is a CSV file
        filepath = os.path.join(directory_path, filename)
        try:
            # Read the CSV file into a DataFrame
            df = pd.read_csv(filepath)

            # Clean the data in the DataFrame (assuming there's a column named 'Dialect')
            # Replace 'Dialect' with the actual column name in your CSV files
            column_name = list(df.columns)[0] # get the first column name

            df = clean_data(df,column_name)

            # Print some info (optional)
            print(f"Processed file: {filename}")
            print(df.head())

            # Save the cleaned DataFrame back to the file, overwriting the original
            df.to_csv(filepath, index=False)

        except Exception as e:
            print(f"Error processing file {filename}: {e}")

import pandas as pd

original_data = pd.read_csv('/content/drive/MyDrive/Final V Project 283/project Draft/longest_captions_dataset -1500 samples.csv')
original_data.head()

import pandas as pd

# Read the four CSV files into pandas DataFrames
egyptain_english_gemini = pd.read_csv('/content/drive/MyDrive/Final V Project 283/Text_transformer_dialects_data/Gemini to English data/egyptain_english_gemini.csv')
khaleji_english_gemini = pd.read_csv('/content/drive/MyDrive/Final V Project 283/Text_transformer_dialects_data/Gemini to English data/khaleji_english_gemini.csv')
saudi_arabia_english_gemini = pd.read_csv('/content/drive/MyDrive/Final V Project 283/Text_transformer_dialects_data/Gemini to English data/saudi_arabia_english_gemini.csv')
moroccan_english_gemini = pd.read_csv('/content/drive/MyDrive/Final V Project 283/Text_transformer_dialects_data/Gemini to English data/moroccan_english_gemini.csv')


egyptain_msa_gemini = pd.read_csv('/content/drive/MyDrive/Final V Project 283/Text_transformer_dialects_data/Gemini to MSA data/egyptain_msa_gemini.csv')
khaleji_msa_gemini = pd.read_csv('/content/drive/MyDrive/Final V Project 283/Text_transformer_dialects_data/Gemini to MSA data/khaleji_msa_gemini.csv')
saudi_arabia_msa_gemini = pd.read_csv('/content/drive/MyDrive/Final V Project 283/Text_transformer_dialects_data/Gemini to MSA data/saudi_arabia_msa_gemini.csv')
moroccan_msa_gemini = pd.read_csv('/content/drive/MyDrive/Final V Project 283/Text_transformer_dialects_data/Gemini to MSA data/moroccan_msa_gemini.csv')

# prompt: for all these data frames name the column "transformed_text"

# Rename columns to 'transformed_text' for all dataframes
egyptain_english_gemini.rename(columns={egyptain_english_gemini.columns[0]: 'transformed_text'}, inplace=True)
khaleji_english_gemini.rename(columns={khaleji_english_gemini.columns[0]: 'transformed_text'}, inplace=True)
saudi_arabia_english_gemini.rename(columns={saudi_arabia_english_gemini.columns[0]: 'transformed_text'}, inplace=True)
moroccan_english_gemini.rename(columns={moroccan_english_gemini.columns[0]: 'transformed_text'}, inplace=True)

egyptain_msa_gemini.rename(columns={egyptain_msa_gemini.columns[0]: 'transformed_text'}, inplace=True)
khaleji_msa_gemini.rename(columns={khaleji_msa_gemini.columns[0]: 'transformed_text'}, inplace=True)
saudi_arabia_msa_gemini.rename(columns={saudi_arabia_msa_gemini.columns[0]: 'transformed_text'}, inplace=True)
moroccan_msa_gemini.rename(columns={moroccan_msa_gemini.columns[0]: 'transformed_text'}, inplace=True)

# prompt: print shapes of all of them

print("egyptain_english_gemini shape:", egyptain_english_gemini.shape)
print("khaleji_english_gemini shape:", khaleji_english_gemini.shape)
print("saudi_arabia_english_gemini shape:", saudi_arabia_english_gemini.shape)
print("moroccan_english_gemini shape:", moroccan_english_gemini.shape)
print("egyptain_msa_gemini shape:", egyptain_msa_gemini.shape)
print("khaleji_msa_gemini shape:", khaleji_msa_gemini.shape)
print("saudi_arabia_msa_gemini shape:", saudi_arabia_msa_gemini.shape)
print("moroccan_msa_gemini shape:", moroccan_msa_gemini.shape)
print("original_data shape:", original_data.shape)

original_data = original_data[:-1]

# prompt: add the two columns that are in the original_data data frane in all these dataframes by this fun : pd.concat([egyptain_english_gemini, original_data], axis=1)

# Concatenate the original data columns with each dataframe
egyptain_english_gemini = pd.concat([egyptain_english_gemini, original_data], axis=1)
khaleji_english_gemini = pd.concat([khaleji_english_gemini, original_data], axis=1)
saudi_arabia_english_gemini = pd.concat([saudi_arabia_english_gemini, original_data], axis=1)
moroccan_english_gemini = pd.concat([moroccan_english_gemini, original_data], axis=1)

egyptain_msa_gemini = pd.concat([egyptain_msa_gemini, original_data], axis=1)
khaleji_msa_gemini = pd.concat([khaleji_msa_gemini, original_data], axis=1)
saudi_arabia_msa_gemini = pd.concat([saudi_arabia_msa_gemini, original_data], axis=1)
moroccan_msa_gemini = pd.concat([moroccan_msa_gemini, original_data], axis=1)

# prompt: now convert all of them to csv file and save in a new folder

import os

# Define the output directory
output_dir = '/content/drive/MyDrive/Final V Project 283/project Draft/Gemini cleaned data'

# Create the output directory if it doesn't exist
os.makedirs(output_dir, exist_ok=True)


# Function to save DataFrame to CSV in the specified directory
def save_to_csv(df, filename):
    filepath = os.path.join(output_dir, filename)
    df.to_csv(filepath, index=False)
    print(f"Saved DataFrame to {filepath}")


# Save each DataFrame to a CSV file in the new folder
save_to_csv(egyptain_english_gemini, 'egyptain_english_gemini.csv')
save_to_csv(khaleji_english_gemini, 'khaleji_english_gemini.csv')
save_to_csv(saudi_arabia_english_gemini, 'saudi_arabia_english_gemini.csv')
save_to_csv(moroccan_english_gemini, 'moroccan_english_gemini.csv')

save_to_csv(egyptain_msa_gemini, 'egyptain_msa_gemini.csv')
save_to_csv(khaleji_msa_gemini, 'khaleji_msa_gemini.csv')
save_to_csv(saudi_arabia_msa_gemini, 'saudi_arabia_msa_gemini.csv')
save_to_csv(moroccan_msa_gemini, 'moroccan_msa_gemini.csv')