# -*- coding: utf-8 -*-
"""Final_StableDiffusion_model_Evaluation_last_version_All_Eval_matrices.ipynb

Automatically generated by Colab.



# **Getting data**
"""

#connect to drive
from google.colab import drive
drive.mount('/content/drive')

# Download data from Kaggel
!kaggle datasets download -d adityajn105/flickr8k
!unzip /content/flickr8k.zip

import os

def count_files_in_folders(root_folder):
  folder_counts = {}
  for foldername, subfolders, filenames in os.walk(root_folder):
    folder_counts[foldername] = len(filenames)
  return folder_counts

root_folder = "/content/drive/MyDrive/Final V Project 283/Stable Diffusion Generated Images‏"
file_counts = count_files_in_folders(root_folder)

for folder, count in file_counts.items():
    print(f"Folder: {folder}, Number of files: {count}")

"""# Manual Enaluation Using Displaying Images

### **Displaying Function**
"""

import os
import matplotlib.pyplot as plt

def display_images_2(original_images_path,generated_images_path_1, generated_images_path_2, start_index=0, end_index=20):
    # List images in directories
    generated_images_1 = os.listdir(generated_images_path_1)
    generated_images_2 = os.listdir(generated_images_path_2)
    original_images = os.listdir(original_images_path)

    # Sort lists to ensure order
    generated_images_1.sort()
    generated_images_2.sort()
    original_images.sort()

    # Validate indices
    num_images = min(len(generated_images_1), len(generated_images_2), len(original_images))
    start_index = max(0, min(start_index, num_images - 1))
    end_index = min(num_images, max(start_index + 1, end_index))

    for i in range(start_index, end_index):
        gen_image_path_1 = os.path.join(generated_images_path_1, generated_images_1[i])
        gen_image_path_2 = os.path.join(generated_images_path_2, generated_images_2[i])
        orig_image_path = os.path.join(original_images_path, original_images[i])

        try:
            img_gen_1 = plt.imread(gen_image_path_1)
            img_gen_2 = plt.imread(gen_image_path_2)
            img_orig = plt.imread(orig_image_path)

            # Display three images side-by-side
            fig, axes = plt.subplots(1, 3, figsize=(25, 5))
            axes[0].imshow(img_orig)
            axes[0].set_title(f"Original - Image {i} - {original_images[i]}")
            axes[0].axis('off')

            axes[1].imshow(img_gen_1)
            axes[1].set_title(f"Gemini English Transformation - Image {i} - {generated_images_1[i]}")
            axes[1].axis('off')

            axes[2].imshow(img_gen_2)
            axes[2].set_title(f"Model 3 English - Image {i} - {generated_images_2[i]}")
            axes[2].axis('off')

            plt.show()

        except Exception as e:
            print(f"Error displaying images at index {i}: {e}")

import os
import matplotlib.pyplot as plt

def display_images(generated_images_path, original_images_path, start_index=0, end_index=20):
    generated_images = os.listdir(generated_images_path)
    original_images = os.listdir(original_images_path)
    # Sort the lists to ensure they are in the same order
    generated_images.sort()
    original_images.sort()
    # Validate indices
    num_images = min(len(generated_images), len(original_images))
    start_index = max(0, min(start_index, num_images - 1))
    end_index = min(num_images, max(start_index + 1, end_index))

    for i in range(start_index, end_index):
        generated_image_path = os.path.join(generated_images_path, generated_images[i])
        original_image_path = os.path.join(original_images_path, original_images[i])
        try:
            img_gen = plt.imread(generated_image_path)
            img_orig = plt.imread(original_image_path)

            fig, axes = plt.subplots(1, 2, figsize=(10, 5))
            axes[0].imshow(img_orig)
            axes[0].set_title(f"Original - Image {i} - {original_images[i]}")
            axes[0].axis('off')

            axes[1].imshow(img_gen)
            axes[1].set_title(f"Generated - Image {i} - {generated_images[i]}")
            axes[1].axis('off')

            plt.show()

        except Exception as e:
            print(f"Error displaying images at index {i}: {e}")

"""#### **Create 1500 samples to test**"""

#  check scipy version

import scipy
print(scipy.__version__)

import os
import shutil
# Create the destination folders
os.makedirs("/content/real_images_1500_sample", exist_ok=True)

original_images = os.listdir("/content/Images")
original_images.sort()

for i in range(1500):
  shutil.copyfile(os.path.join("/content/Images", original_images[i]), os.path.join("real_images_1500_sample", original_images[i]))

#  count this folder /content/images_1000_sample

!ls /content/real_images_1500_sample | wc -l

"""# **Original Images vs. Generated Images Display**"""

# read original data /content/real_images_1500_sample in variable
original_images_path = "/content/real_images_1500_sample"

"""## Model 3 Faseeh Transformation to English Images"""

egyptian_images_path_model_3 = "/content/drive/MyDrive/Final V Project 283/Stable Diffusion Generated Images‏/Egyptain Dialect Generated images/generated_egyptain_images_model3"  # Replace with the actual path
khaleji_images_path_model_3 = "/content/drive/MyDrive/Final V Project 283/Stable Diffusion Generated Images‏/khaleeji_generated_images/generated_khaleeji_images_model3"  # Replace with the actual path
saudi_images_path_model_3 = "/content/drive/MyDrive/Final V Project 283/Stable Diffusion Generated Images‏/saudi_generated_images/generated_saudi_images_model3"  # Replace with the actual path
moroccan_images_path_model_3 = "/content/drive/MyDrive/Final V Project 283/Stable Diffusion Generated Images‏/morrocon_generated_images/generated_morrocon_images_model3"  # Replace with the actual path

"""## Model 4 Gemini Transformation to English Images"""

egyptian_images_path_model_4_english = "/content/drive/MyDrive/Final V Project 283/Stable Diffusion Generated Images‏/Egyptain Dialect Generated images/generated_egyptain_english_gemini_images"  # Replace with the actual path
khaleji_images_path_model_4_english  = "/content/drive/MyDrive/Final V Project 283/Stable Diffusion Generated Images‏/khaleeji_generated_images/generated_khaleeji_images_model4_eng"  # Replace with the actual path
saudi_images_path_model_4_english  = "/content/drive/MyDrive/Final V Project 283/Stable Diffusion Generated Images‏/saudi_generated_images/generated_saudi_images_model4_eng"  # Replace with the actual path
moroccan_images_path__model_4_english = "/content/drive/MyDrive/Final V Project 283/Stable Diffusion Generated Images‏/morrocon_generated_images/generated_morrocon_images_model4_eng"  # Replace with the actual path

"""## Egyptain"""

display_images_2(original_images_path,egyptian_images_path_model_4_english, egyptian_images_path_model_3,1200, 1210)

"""## khaleji"""

display_images_2(original_images_path,khaleji_images_path_model_4_english, khaleji_images_path_model_3, 20, 40)

"""## saudi"""

display_images_2(original_images_path, saudi_images_path_model_4_english, saudi_images_path_model_3, 20, 40)

"""## moroccan"""

display_images_2(original_images_path, moroccan_images_path__model_4_english,moroccan_images_path_model_3 , 560, 580)

"""# **Functions for Evaluation**

#### **FID SCORE**
"""

!pip install pytorch-fid

!pip install torchvision

import os
import pathlib
import numpy as np
import torch
import torchvision.transforms as TF
from PIL import Image
from scipy import linalg
from torch.nn.functional import adaptive_avg_pool2d
from argparse import ArgumentDefaultsHelpFormatter, ArgumentParser
import pytorch_fid
from pytorch_fid.inception import InceptionV3
try:
    from tqdm import tqdm
except ImportError:
    # If tqdm is not available, provide a mock version of it
    def tqdm(x):
        return x
# Add the import statement for pathlib
import pathlib
IMAGE_EXTENSIONS = {"bmp", "jpg", "jpeg", "pgm", "png", "ppm", "tif", "tiff", "webp"}

class ImagePathDataset(torch.utils.data.Dataset):
    def __init__(self, files, transforms=None):
        self.files = files
        self.transforms = transforms
    def __len__(self):
        return len(self.files)

    def __getitem__(self, i):
        path = self.files[i]
        img = Image.open(path).convert("RGB")
        if self.transforms is not None:
            img = self.transforms(img)
        return img

def get_activations(files, model, batch_size=50, dims=2048, device="cpu", num_workers=1):
    model.eval()
    if batch_size > len(files):
        print("Warning: batch size is bigger than the data size. Setting batch size to data size")
        batch_size = len(files)
    # Ensure images are resized and normalized
    transform = TF.Compose([
        TF.Resize((299, 299)),
        TF.ToTensor(),
        TF.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),
    ])
    dataset = ImagePathDataset(files, transforms=transform)
    dataloader = torch.utils.data.DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=False,
        drop_last=False,
        num_workers=num_workers,
    )
    pred_arr = np.empty((len(files), dims))
    start_idx = 0

    for batch in tqdm(dataloader):
        batch = batch.to(device)
        # Print shape of batch for debugging
        print(f"Batch shape: {batch.shape}")

        with torch.no_grad():
            pred = model(batch)[0]

        if pred.size(2) != 1 or pred.size(3) != 1:
            pred = adaptive_avg_pool2d(pred, output_size=(1, 1))

        pred = pred.squeeze(3).squeeze(2).cpu().numpy()

        pred_arr[start_idx:start_idx + pred.shape[0]] = pred

        start_idx = start_idx + pred.shape[0]

    return pred_arr
import numpy as np
from scipy import linalg

def calculate_frechet_distance(mu1, sigma1, mu2, sigma2, eps=1e-6):
    mu1 = np.atleast_1d(mu1)
    mu2 = np.atleast_1d(mu2)
    sigma1 = np.atleast_2d(sigma1)
    sigma2 = np.atleast_2d(sigma2)
    diff = mu1 - mu2
    # Product might be almost singular
    covmean, _ = linalg.sqrtm(sigma1.dot(sigma2), disp=False)

    # Adding the epsilon to the diagonal of covmean to ensure numerical stability
    if not np.isfinite(covmean).all():
        offset = np.eye(sigma1.shape[0]) * eps
        covmean = linalg.sqrtm((sigma1 + offset).dot(sigma2 + offset))

    if np.iscomplexobj(covmean):
        if not np.allclose(np.diagonal(covmean).imag, 0, atol=1e-3):
            m = np.max(np.abs(covmean.imag))
            raise ValueError(f"Imaginary component {m}")
        covmean = covmean.real

    tr_covmean = np.trace(covmean)

    return (diff.dot(diff) + np.trace(sigma1) + np.trace(sigma2) - 2 * tr_covmean)


def calculate_activation_statistics(files, model, batch_size=50, dims=2048, device="cpu", num_workers=1):
    act = get_activations(files, model, batch_size, dims, device, num_workers)
    mu = np.mean(act, axis=0)
    sigma = np.cov(act, rowvar=False)
    return mu, sigma

def compute_statistics_of_path(path, model, batch_size, dims, device, num_workers=1):
    if path.endswith(".npz"):
        with np.load(path) as f:
            m, s = f["mu"][:], f["sigma"][:]
    else:
        path = pathlib.Path(path)
        files = sorted([file for ext in IMAGE_EXTENSIONS for file in path.glob(f'*.{ext}')])
        m, s = calculate_activation_statistics(files, model, batch_size, dims, device, num_workers)

    return m, s

def calculate_fid_given_paths(paths, batch_size, device, dims, num_workers=1):
    for p in paths:
        if not os.path.exists(p):
            raise RuntimeError("Invalid path: %s" % p)

    block_idx = InceptionV3.BLOCK_INDEX_BY_DIM[dims]

    model = InceptionV3([block_idx]).to(device)

    m1, s1 = compute_statistics_of_path(paths[0], model, batch_size, dims, device, num_workers)
    m2, s2 = compute_statistics_of_path(paths[1], model, batch_size, dims, device, num_workers)
    fid_value = calculate_frechet_distance(m1, s1, m2, s2)

    return fid_value

"""#### IS Evaluation"""

from math import floor
from numpy import ones
from numpy import expand_dims
from numpy import log
from numpy import mean
from numpy import std
from numpy import exp
from numpy.random import shuffle
from keras.applications.inception_v3 import InceptionV3
from keras.applications.inception_v3 import preprocess_input
from skimage.transform import resize
from numpy import asarray
import os
import PIL
from PIL import Image
import cv2
# scale an array of images to a new size
def scale_images(images, new_shape):
    images_list = list()
    for image in images:
        # resize with nearest neighbor interpolation
        new_image = resize(image, new_shape, 0)
        # store
        images_list.append(new_image)
    return asarray(images_list)

# assumes images have any shape and pixels in [0,255]
!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
!pip install numpy


import torch
import torchvision.models as models
from torch.utils.data import DataLoader, Dataset
from torchvision import transforms
import numpy as np


import torch
import torchvision.models as models
from torch.utils.data import DataLoader, Dataset
from torchvision import transforms
import numpy as np

import torch
import torchvision.models as models
from torch.utils.data import DataLoader, Dataset
from torchvision import transforms
import numpy as np

def calculate_inception_score(images, n_split=10, eps=1E-16):
    """
    Calculates the Inception Score of the generated images.

    Args:
        images (list of PIL Images): A list of PIL Images representing the generated images.
        n_split (int): The number of splits to use for calculating the Inception Score.
        eps (float): A small value to avoid division by zero.

    Returns:
        tuple: (Inception Score mean, Inception Score standard deviation)
    """
    # Load a pre-trained InceptionV3 model
    model = models.inception_v3(pretrained=True, transform_input=False)
    model.eval()

    # Move model to GPU if available
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)

    # Preprocess the images
    preprocess = transforms.Compose([
        transforms.Resize((299, 299)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406],  # ImageNet mean
                             std=[0.229, 0.224, 0.225]),  # ImageNet std
    ])

    # Create a DataLoader for the images
    class ImageDataset(Dataset):
        def __init__(self, images, preprocess):
            self.images = images
            self.preprocess = preprocess

        def __len__(self):
            return len(self.images)

        def __getitem__(self, idx):
            image = self.images[idx]
            image = self.preprocess(image)
            return image

    dataset = ImageDataset(images, preprocess)
    dataloader = DataLoader(dataset, batch_size=32, shuffle=False, num_workers=4)

    # Calculate the Inception Score
    scores = []
    for batch in dataloader:
        batch = batch.to(device)
        with torch.no_grad():
            # Get predictions
            outputs = model(batch)
            # Calculate probabilities
            p_yx = torch.nn.functional.softmax(outputs, dim=1).cpu().numpy()
            # Calculate p(y)
            p_y = np.expand_dims(p_yx.mean(axis=0), 0)
            # Calculate KL divergence
            kl_d = p_yx * (np.log(p_yx + eps) - np.log(p_y + eps))
            # Sum KL divergence across classes
            sum_kl_d = kl_d.sum(axis=1)
            # Average KL divergence across images in the batch
            avg_kl_d = sum_kl_d.mean()
            # Calculate Inception Score for the batch
            is_score = np.exp(avg_kl_d)
            scores.append(is_score)

    # Calculate the final Inception Score
    is_avg = np.mean(scores)
    is_std = np.std(scores)

    return is_avg, is_std  # Return both mean and standard deviation


# Function to load images from the directory

def load_images_from_folder(folder):
    images = []
    target_size = (299, 299)  # InceptionV3 expects 299x299 images
    for filename in os.listdir(folder):
        file_path = os.path.join(folder, filename)
        # Ensure you're reading only image files
        if not os.path.isfile(file_path):
            continue
        try:
            img = cv2.imread(file_path)
            if img is not None:
                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB
                img = cv2.resize(img, target_size)
                img_pil = Image.fromarray(img)  # Convert NumPy array to PIL Image
                images.append(img_pil)
        except Exception as e:
            print(f"Error loading image {file_path}: {e}")
    return images

from pytorch_fid.inception import InceptionV3
# Define the paths to the real and generated images
real_images_path = "/content/real_images_1500_sample"
# Set parameters
batch_size = 50
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
dims = 2048
num_workers = 4



"""#### **CLIP SCORE Function**"""

!pip install flagai
!pip install torchvision

# Load model directly
from transformers import AutoProcessor, AutoModelForZeroShotImageClassification

processor = AutoProcessor.from_pretrained("BAAI/AltCLIP-m18")
model = AutoModelForZeroShotImageClassification.from_pretrained("BAAI/AltCLIP-m18")

import os
import torch
from PIL import Image
from tqdm import tqdm
from torch.nn.functional import cosine_similarity
import numpy as np

def compute_clip_similarity_in_batches(image_folder, captions, processor, model, batch_size=32, device="cuda"):
    """
    Calculate cosine similarity scores for text-image pairs using AltCLIP-m18 in batches.

    Args:
        image_folder (str): Path to the folder containing images.
        captions (list): List of text captions corresponding to the images.
        processor: The AltCLIP processor (from Hugging Face).
        model: The AltCLIP model (from Hugging Face).
        batch_size (int): Number of images and captions to process per batch.
        device (str): Device to run the model on ("cuda" or "cpu").

    Returns:
        dict: A dictionary with individual scores, the similarity matrix, and the mean score.
    """
    # Set up device
    device = torch.device(device if torch.cuda.is_available() else "cpu")
    model.to(device)

    # Load image paths
    image_paths = sorted([os.path.join(image_folder, img) for img in os.listdir(image_folder) if img.endswith(('.png', '.jpg', '.jpeg'))])
    if len(image_paths) != len(captions):
        raise ValueError("The number of images and captions must match.")

    # Initialize results
    all_individual_scores = []
    similarity_matrices = []

    # Process in batches
    for i in tqdm(range(0, len(image_paths), batch_size), desc="Processing Batches"):
        batch_image_paths = image_paths[i:i + batch_size]
        batch_captions = captions[i:i + batch_size]

        # Load and preprocess images
        images = [Image.open(img_path).convert("RGB") for img_path in batch_image_paths]

        # Encode images and captions
        with torch.no_grad():
            # Process and encode images
            processed_images = processor(images=images, return_tensors="pt").to(device)
            image_embeddings = model.get_image_features(**processed_images)

            # Process and encode text captions
            processed_texts = processor(text=batch_captions, return_tensors="pt", padding=True, truncation=True).to(device)
            text_embeddings = model.get_text_features(**processed_texts)

        # Normalize embeddings
        image_embeddings = image_embeddings / image_embeddings.norm(dim=-1, keepdim=True)
        text_embeddings = text_embeddings / text_embeddings.norm(dim=-1, keepdim=True)

        # Calculate cosine similarity
        batch_similarity_matrix = torch.mm(text_embeddings, image_embeddings.T).cpu().numpy()

        # Extract individual scores (diagonal of the similarity matrix)
        batch_individual_scores = np.diagonal(batch_similarity_matrix).tolist()

        # Append batch results
        all_individual_scores.extend(batch_individual_scores)
        similarity_matrices.append(batch_similarity_matrix)

    # Concatenate similarity matrices
    similarity_matrix = np.vstack([np.pad(matrix, ((0, 0), (0, batch_size - matrix.shape[1])), constant_values=0)
                                    for matrix in similarity_matrices])

    # Compute mean score
    mean_score = np.mean(all_individual_scores)

    return {
        "similarity_matrix": similarity_matrix,
        "individual_scores": all_individual_scores,
        "mean_score": mean_score
    }

"""# **FID Score Evaluation**

## Khaleji  FID Evaluation
"""

Fid_Value_khaleji_images_model_4_english = calculate_fid_given_paths([real_images_path, khaleji_images_path_model_4_english], batch_size, device, dims, num_workers)
Fid_Value_khaleeji_images_model_3 = calculate_fid_given_paths([real_images_path, khaleji_images_path_model_3], batch_size, device, dims, num_workers)

print("FID khaleeji_images_ model 3: ", Fid_Value_khaleeji_images_model_3)
print("FID khaleji_images_model 4 gemini english: ", Fid_Value_khaleji_images_model_4_english)

"""## Egyptain  FID Evaluation"""

#  fid score evaluation for Egyptain dialect images
Fid_Value_egyptian_images_model_3 = calculate_fid_given_paths([real_images_path, egyptian_images_path_model_3], batch_size, device, dims, num_workers)
Fid_Value_egyptian_images_model_4_english = calculate_fid_given_paths([real_images_path, egyptian_images_path_model_4_english], batch_size, device, dims, num_workers)

print("FID egyptian_images_model 3: ", Fid_Value_egyptian_images_model_3)
print("FID egyptian_images_model 4 Gemini english: ", Fid_Value_egyptian_images_model_4_english)

"""## Morrocan FID Evaluation"""

# Calculate FID score for the Morrocan image sets
Fid_Value_morrocan_images_model_3 = calculate_fid_given_paths([real_images_path, moroccan_images_path_model_3], batch_size, device, dims, num_workers)
Fid_Value_moroccan_images_model_4_english = calculate_fid_given_paths([real_images_path, moroccan_images_path__model_4_english], batch_size, device, dims, num_workers)

print("FID morrocan_images_model 3: ", Fid_Value_morrocan_images_model_3)
print("FID morrocan_images_model 4 gemini english: ", Fid_Value_moroccan_images_model_4_english)

"""## saudi-arabia  FID Evaluation"""

Fid_Value_saudi_images_model_3 = calculate_fid_given_paths([real_images_path, saudi_images_path_model_3], batch_size, device, dims, num_workers)
Fid_Value_saudi_images_model_4_english = calculate_fid_given_paths([real_images_path, saudi_images_path_model_4_english], batch_size, device, dims, num_workers)

print("FID saudi_images_model 3: ", Fid_Value_saudi_images_model_3)
print("FID saudi_images_model 4 Gemini english: ", Fid_Value_saudi_images_model_4_english)

"""# **IS Score Evaluation**

## Khaleji  IS Evaluation
"""

IS_Value_khaleeji_images_model_3 = calculate_inception_score(load_images_from_folder(khaleji_images_path_model_3))
IS_Value_khaleeji_images_model_4_english = calculate_inception_score(load_images_from_folder(khaleji_images_path_model_4_english))

print("IS khaleeji_images_ model 3: ", IS_Value_khaleeji_images_model_3)
print("IS khaleeji_images_model 4 gemini english: ", IS_Value_khaleeji_images_model_4_english)

"""## Egyptain IS Evaluation"""

# Calculate Inception score for these images
IS_Value_egyptain_images_model_3 = calculate_inception_score(load_images_from_folder(egyptian_images_path_model_3))
IS_Value_egyptain_images_model_4_english = calculate_inception_score(load_images_from_folder(egyptian_images_path_model_4_english))

print("IS egyptian_images_model 3: ", IS_Value_egyptain_images_model_3)
print("IS egyptian_images_model 4 gemini english: ", IS_Value_egyptain_images_model_4_english)

"""## Morrocan IS Evaluation"""

IS_Value_moroccan_images_model_3 = calculate_inception_score(load_images_from_folder(moroccan_images_path_model_3))
IS_Value_moroccan_images_model_4_english = calculate_inception_score(load_images_from_folder(moroccan_images_path__model_4_english))

print("IS morrocan_images_model 3: ", IS_Value_moroccan_images_model_3)
print("IS morrocan_images_model 4 gemini english: ", IS_Value_moroccan_images_model_4_english)

"""## saudi-arabia  IS Evaluation"""

IS_Value_saudi_images_model_3 = calculate_inception_score(load_images_from_folder(saudi_images_path_model_3))
IS_Value_saudi_images_model_4_english = calculate_inception_score(load_images_from_folder(saudi_images_path_model_4_english))

print("IS saudi_images_model 3: ", IS_Value_saudi_images_model_3)
print("IS saudi_images_model 4 gemini english: ", IS_Value_saudi_images_model_4_english)

"""# **CLIP Score Evaluation**"""

import pandas as pd
original_df=pd.read_csv("/content/drive/MyDrive/Final V Project 283/project Draft/longest_captions_dataset -1500 samples.csv")

original_df.head()

original_text = original_df['text'].tolist()
len(original_text)

original_text = original_text[:1500]

"""#### Clip score for real images with original text is **0.33232479652265706**

## Khaleji  CLIP Score Evaluation
"""

# apply compute_clip_similarity_in_batches function to khaleji images model 3 and 4 and original text
khaleji_clip_model_3 = compute_clip_similarity_in_batches(khaleji_images_path_model_3, original_text, processor, model,batch_size=32)
khaleji_clip_model_4 = compute_clip_similarity_in_batches(khaleji_images_path_model_4_english, original_text, processor, model,batch_size=32)

print("khaleji_clip_model_3: ", khaleji_clip_model_3.get('mean_score'))
print("khaleji_clip_model_4: ", khaleji_clip_model_4.get('mean_score'))

"""## Egyptain CLIP Score Evaluation"""

egyptian_clip_model_3 = compute_clip_similarity_in_batches(egyptian_images_path_model_3, original_text, processor, model,batch_size=32)
egyptian_clip_model_4 = compute_clip_similarity_in_batches(egyptian_images_path_model_4_english, original_text, processor, model,batch_size=32)

print("egyptian_clip_model_3: ", egyptian_clip_model_3.get('mean_score'))
print("egyptian_clip_model_4: ", egyptian_clip_model_4.get('mean_score'))

"""## Morrocan CLIP Score Evaluation"""

moroccan_clip_model_3 = compute_clip_similarity_in_batches(moroccan_images_path_model_3, original_text, processor, model, batch_size=32)
moroccan_clip_model_4 = compute_clip_similarity_in_batches(moroccan_images_path__model_4_english, original_text, processor, model, batch_size=32)

print("moroccan_clip_model_3: ", moroccan_clip_model_3.get('mean_score'))
print("moroccan_clip_model_4: ", moroccan_clip_model_4.get('mean_score'))

"""## saudi-arabia  CLIP Score Evaluation"""

saudi_clip_model_3 = compute_clip_similarity_in_batches(saudi_images_path_model_3, original_text, processor, model,batch_size=32)
saudi_clip_model_4 = compute_clip_similarity_in_batches(saudi_images_path_model_4_english, original_text, processor, model,batch_size=32)

print("saudi_clip_model_3: ", saudi_clip_model_3.get('mean_score'))
print("saudi_clip_model_4: ", saudi_clip_model_4.get('mean_score'))