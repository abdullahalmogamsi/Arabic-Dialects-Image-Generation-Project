# -*- coding: utf-8 -*-
"""Final_Testing_arabic_dialects_project_Interface_with_MSA_transformation_gradio.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xZgoJBTjXn8h2ByCfsNOIJEFdtTa6EsY

## **Install Libaries**
"""

!pip install langchain_google_genai
!pip install langchain

pip install diffusers==0.24.0

# !pip install git+https://github.com/huggingface/diffusers.git torch transformers sentencepiece

#  diffusers==0.24.0
!pip install diffusers torch transformers sentencepiece

pip show diffusers

from google.colab import userdata
HF_Token_faisl = userdata.get('HF_Token_faisl')

!huggingface-cli login --token $HF_Token_faisl

!pip install -U bitsandbytes

!pip install gradio

"""## **Transformation to Arabic Models**"""

# Load model directly
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

tokenizer_1 = AutoTokenizer.from_pretrained("PRAli22/arat5-arabic-dialects-translation")
model_1 = AutoModelForSeq2SeqLM.from_pretrained("PRAli22/arat5-arabic-dialects-translation")

# Load model directly
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
model_name = "Murhaf/AraT5-MSAizer"
tokenizer_2 = AutoTokenizer.from_pretrained(model_name)
model_2 = AutoModelForSeq2SeqLM.from_pretrained(model_name)

# Load model directly
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
tokenizer_3 = AutoTokenizer.from_pretrained("nadsoft/Faseeh-v0.1-beta")
model_3 = AutoModelForSeq2SeqLM.from_pretrained("nadsoft/Faseeh-v0.1-beta")

# create function to transform text
def transform_text_model_1(input_text):
    inputs = tokenizer_1(input_text, return_tensors="pt")
    outputs = model_1.generate(**inputs)

    # Decode the outputs and print the results
    decoded_sentences = tokenizer_1.batch_decode(outputs, skip_special_tokens=True)
    return decoded_sentences[0]

# create function to transform text for model 1 atat5
def transform_text_model_2(input_text):
    inputs = tokenizer_2(input_text, return_tensors="pt")
    outputs = model_2.generate(**inputs)
    # Decode the outputs and print the results
    decoded_sentences = tokenizer_2.batch_decode(outputs, skip_special_tokens=True)
    return decoded_sentences[0]

# create function to transform text for model 3 faseeh
def transform_text_model_3(input_text):
    inputs = tokenizer_3(input_text, return_tensors="pt")
    outputs = model_3.generate(**inputs)
    # Decode the outputs and print the results
    decoded_sentences = tokenizer_3.batch_decode(outputs, skip_special_tokens=True)
    return decoded_sentences[0]

transform_text_model_2(" رجال نايم على دكة برة مع كلب أبيض جالس جنبه ")

transform_text_model_3(" كلب بني كيجرى ف حديقة قريب من خرطوم ديال الحديقة.")

transform_text_model_1(" رجال نايم على دكة برة مع كلب كحلي جالس جنبه ")

"""## **Gemini Arabic and English Functions**"""

from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.prompts import PromptTemplate
import google.generativeai as genai
import pandas as pd
from langchain.schema import HumanMessage

model = genai.GenerativeModel(model_name = "gemini-1.5-flash-latest")

google_api_key='AIzaSyB9h8ue38uvwiqRh7htpUAPRrZmi6Ei9Ug'

def transform_text_gemini_msa(input_text):
    llm = ChatGoogleGenerativeAI(model="gemini-1.5-flash-latest", google_api_key=google_api_key)
    template = """
اعمل كمترجم من اللهجات العربيه العاميه إلى اللغه العربيه الفصحى وأجب فقط بالشكل المطلوب دون إضافة كلمات إضافية.
الجمله بالللهجه العاميه :
{arabic_sentences}
اللغه العربيه الفصحى:
"""
    prompt_template = PromptTemplate(input_variables=["arabic_sentences"], template=template)
    ques = prompt_template.format(arabic_sentences=input_text)
    result = llm([HumanMessage(content=ques)])
    return result.content

def transform_text_gemini_english(input_text):
    llm = ChatGoogleGenerativeAI(model="gemini-1.5-flash-latest", google_api_key=google_api_key)
    template = """
اعمل كمترجم من اللهجات العربيه العاميه إلى اللغه الانجليزيه وأجب فقط بالشكل المطلوب دون إضافة كلمات إضافية.
الجمله بالللهجه العاميه :
{arabic_sentences}
اللغه الانجليزيه:
"""
    prompt_template = PromptTemplate(input_variables=["arabic_sentences"], template=template)
    ques = prompt_template.format(arabic_sentences=input_text)
    result = llm([HumanMessage(content=ques)])
    return result.content

test_msa=transform_text_gemini_msa(" رجال نايم على دكة برة مع كلب كحلي جالس جنبه ")
test_msa

test_eng=transform_text_gemini_english(" رجال نايم على دكة برة مع كلب كحلي جالس جنبه ")
test_eng

"""# **Function and model load for Generating images**

## **Alt Diffusion**
"""

file_path = '../usr/local/lib/python3.10/dist-packages/diffusers/utils/dynamic_modules_utils.py'

# Read the file content.
try:
    with open(file_path, 'r') as file:
        file_content = file.read()
except FileNotFoundError:
    print(f"Error: File '{file_path}' not found.")
    exit(1)


# Perform the replacement.
new_file_content = file_content.replace("cached_download,", "")


# Write the modified content back to the file.
try:
    with open(file_path, 'w') as file:
        file.write(new_file_content)
    print(f"Successfully replaced 'cached_download,' in '{file_path}'")
except Exception as e:
    print(f"Error writing to file '{file_path}': {e}")
    exit(1)

from diffusers import AltDiffusionPipeline, DPMSolverMultistepScheduler
import torch
# /content/diffusers_2_4

if torch.cuda.is_available():
  pipe = AltDiffusionPipeline.from_pretrained("BAAI/AltDiffusion-m9", torch_dtype=torch.float16,safety_checker=None) # last add safety checker
  pipe = pipe.to("cuda")
else:
  pipe = AltDiffusionPipeline.from_pretrained("BAAI/AltDiffusion-m9",safety_checker=None) # last add safety checker

pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)

# prompt: generate image generator function that takes two parameters the number of images I want and the caption that generate the image

def generate_images_alt(num_images, caption):
    images = []
    for _ in range(num_images):
        image = pipe(caption, num_inference_steps=50, guidance_scale=7.5).images[0]
        images.append(image)
    return images

"""## Stable Diffusion"""

!pip install transformers
import transformers

transformers.utils.move_cache()

from gradio_client import Client
from PIL import Image
import matplotlib.pyplot as plt

client_stable = Client("stabilityai/stable-diffusion-3-medium",hf_token= HF_Token_faisl)
def stable_pipeline(caption, num_inference_steps_in=28, guidance_scale_in=4.5):
  result = client_stable.predict(
      prompt=caption,
      negative_prompt="blurry",
      seed=0,
      randomize_seed=True,
      width=512,
      height=512,
      guidance_scale=guidance_scale_in,
      num_inference_steps=num_inference_steps_in,
      api_name="/infer"
  )

  img = Image.open(result[0])

  return img

prompt = "a girl wearing red jacket"

image = stable_pipeline(prompt, num_inference_steps_in=25)#.images[0]
image.save("girl_stable.png")

image

def generate_images_stable(num_images, caption):
    images = []
    for _ in range(num_images):
        image = stable_pipeline(caption, num_inference_steps_in=25, guidance_scale_in=4.5)#.images[0]
        images.append(image)
    return images

"""## Flux Model"""

from gradio_client import Client
from PIL import Image
import matplotlib.pyplot as plt

client_flux = Client("black-forest-labs/FLUX.1-schnell",hf_token=HF_Token_faisl)
def flux_pipe(caption, num_inference_steps_in=4, guidance_scale_in=3.5):
  result = client_flux.predict(
      prompt=caption,
      seed=0,
      randomize_seed=True,
      width=1024,
      height=1024,
      num_inference_steps=num_inference_steps_in,
      api_name="/infer"
  )

  img = Image.open(result[0])

  return img

flux_pipe


prompt = "a girl wearing red jacket"

image = flux_pipe(prompt, num_inference_steps_in=4)#.images[0]
image.save("girl_stable_flux.png")

image

def generate_images_flux(num_images, caption):
    images = []
    for _ in range(num_images):
        image = flux_pipe(caption, num_inference_steps_in=4)#.images[0]
        images.append(image)
    return images

"""# **Gradio Interface**"""

import gradio as gr

def generate_and_display_images(prompt, num_images, img_gen_model, text_transform_model_alt, text_transform_model_flux_stable):
        # Choose the appropriate text transform model based on the image generation model
        if img_gen_model == "Alt Diffusion":
            text_transform_model = text_transform_model_alt
        else:
            text_transform_model = text_transform_model_flux_stable

        # Apply text transformation
        if img_gen_model == "Alt Diffusion":
            if text_transform_model == "Model 1":
                transformed_prompt = transform_text_model_1(prompt)
            elif text_transform_model == "Model 2":
                transformed_prompt = transform_text_model_2(prompt)
            elif text_transform_model == "Model 3":
                transformed_prompt = transform_text_model_3(prompt)
            elif text_transform_model == "Gemini MSA":
                transformed_prompt = transform_text_gemini_msa(prompt)
            elif text_transform_model == "Gemini English":
                transformed_prompt = transform_text_gemini_english(prompt)
        elif img_gen_model in ["Stable Diffusion", "Flux"]:
            if text_transform_model == "Gemini English":
                transformed_prompt = transform_text_gemini_english(prompt)
            elif text_transform_model == "Model 3":
                transformed_prompt = transform_text_model_3(prompt)
        else:
            transformed_prompt = prompt  # Fallback in case of no transformation

        # Generate images without transformation
        alt_images_no_transform = generate_images_alt(num_images, prompt)

        # Generate images after transformation
        if img_gen_model == "Alt Diffusion":
            generated_images = generate_images_alt(num_images, transformed_prompt)
        elif img_gen_model == "Stable Diffusion":
            generated_images = generate_images_stable(num_images, transformed_prompt)
        elif img_gen_model == "Flux":
            generated_images = generate_images_flux(num_images, transformed_prompt)

        # Return images and transformed prompt
        return alt_images_no_transform, generated_images, transformed_prompt

import gradio as gr

# Gradio interface components
with gr.Blocks(theme='lone17/kotaemon') as app:
    # Logo and Title
    with gr.Row(equal_height=True):
        with gr.Column(scale=1, min_width=80):
            gr.Image(
                "https://upload.wikimedia.org/wikipedia/en/thumb/7/70/KAUST_Logo.svg/1200px-KAUST_Logo.svg.png",
                elem_id="logo",
                show_label=False,
                height=150,
                width=150 , # Prevent stretching
                show_download_button= False,
                show_fullscreen_button= False
            )
        with gr.Column(scale=5):
            gr.Markdown(
                "<h1 style='text-align: center; margin: 0; color: #0073e6; font-size: 48px; '>Arabic Dialects Image Generation Project</h1>",
                elem_id="title"
            )

    # Introduction
    gr.Markdown("""
    Welcome to the Arabic Dialects Image Generation interface. Here you can:
    - Enter a prompt in any Arabic dialect.
    - Choose an image generation model (Alt Diffusion, Stable Diffusion, or Flux).
    - Optionally apply a text transformation model to refine your input prompt.

    The interface will generate two sets of images: one without transformation and one with your selected transformation.
    """)

    # Inputs
    with gr.Row():
        prompt = gr.Textbox(label="Prompt", placeholder="Enter your text here...")
        num_images = gr.Slider(minimum=1, maximum=4, step=1, label="Number of Images", value=1)

    with gr.Row():
        img_gen_model = gr.Dropdown(
            label="Image Generation Model",
            choices=["Alt Diffusion", "Stable Diffusion", "Flux"],
            value="Alt Diffusion"
        )
        text_transform_alt = gr.Dropdown(
            label="Text Transformation Model (Alt Diffusion)",
            choices=[
                "Model 1", "Model 2", "Model 3",
                "Gemini MSA", "Gemini English"
            ],
            value="Model 1",
            visible=True
        )
        text_transform_flux_stable = gr.Dropdown(
            label="Text Transformation Model (Stable Diffusion/Flux)",
            choices=["Gemini English", "Model 3"],
            value="Gemini English",
            visible=False
        )

    # Button to generate images
    generate_button = gr.Button("Generate")

    # Outputs
    with gr.Row():
        images_output_without_transform = gr.Gallery(
            label="AltDiffusion Generated Images (Without Text Transformation)", scale=3, height="300px")
        images_output_with_transform = gr.Gallery(
            label="Generated Images after Text Transformation", scale=3, height="300px")
    transformed_prompt_output = gr.Textbox(label="Transformed Prompt", interactive=False)

    # Function to toggle visibility of text transformation dropdowns
    def toggle_dropdowns(img_gen_model):
        if img_gen_model == "Alt Diffusion":
            return gr.update(visible=True), gr.update(visible=False)
        else:
            return gr.update(visible=False), gr.update(visible=True)

    img_gen_model.change(
        toggle_dropdowns,
        inputs=[img_gen_model],
        outputs=[text_transform_alt, text_transform_flux_stable]
    )


    generate_button.click(
        generate_and_display_images,
        inputs=[prompt, num_images, img_gen_model, text_transform_alt, text_transform_flux_stable],
        outputs=[
            images_output_without_transform,
            images_output_with_transform,
            transformed_prompt_output
        ]
    )

app.launch(share=True, debug=True)

